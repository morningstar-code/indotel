"""
Script para extraer datos reales de REGULATEL
Extrae informaci√≥n de las 8 categor√≠as regulatorias para cada pa√≠s
"""
import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
import time

BASE_URL = "https://regulatel.indotel.gob.do"

# Mapeo de pa√≠ses y sus URLs
COUNTRIES = {
    "argentina": {"name": "Argentina", "flag": "üá¶üá∑", "url": "argentina"},
    "bolivia": {"name": "Bolivia", "flag": "üáßüá¥", "url": "bolivia"},
    "brasil": {"name": "Brasil", "flag": "üáßüá∑", "url": "brasil"},
    "chile": {"name": "Chile", "flag": "üá®üá±", "url": "chile"},
    "colombia": {"name": "Colombia", "flag": "üá®üá¥", "url": "colombia"},
    "ecuador": {"name": "Ecuador", "flag": "üá™üá®", "url": "ecuador"},
    "mexico": {"name": "M√©xico", "flag": "üá≤üáΩ", "url": "mexico"},
    "paraguay": {"name": "Paraguay", "flag": "üáµüáæ", "url": "paraguay"},
    "peru": {"name": "Per√∫", "flag": "üáµüá™", "url": "peru"},
    "rep_dominicana": {"name": "Rep√∫blica Dominicana", "flag": "üá©üá¥", "url": "rep-dominicana"},
    "uruguay": {"name": "Uruguay", "flag": "üá∫üáæ", "url": "uruguay"},
}

CATEGORIES = [
    "Espectro radioel√©ctrico",
    "Competencia Econ√≥mica",
    "Ciberseguridad",
    "Protecci√≥n del usuario",
    "Tecnolog√≠as emergentes",
    "Compartici√≥n de la infraestructura",
    "Telecomunicaciones de emergencia",
    "Homologaci√≥n de productos y dispositivos",
]

def extract_tags(text):
    """Extrae tags relevantes del texto bas√°ndose en palabras clave"""
    tags = []
    text_lower = text.lower()
    
    keyword_map = {
        "subastas": ["subasta", "licitaci√≥n", "asignaci√≥n competitiva"],
        "5g": ["5g", "quinta generaci√≥n", "5 g"],
        "omv": ["omv", "operador m√≥vil virtual", "operadores m√≥viles virtuales"],
        "portabilidad": ["portabilidad", "portable", "portaci√≥n"],
        "ciberseguridad": ["ciberseguridad", "cybersecurity", "seguridad digital", "cibern√©tico"],
        "iot": ["iot", "internet de las cosas", "internet of things"],
        "infraestructura": ["infraestructura", "compartici√≥n", "compartir infraestructura"],
        "emergencia": ["emergencia", "alertas", "sistema de alerta"],
        "homologaci√≥n": ["homologaci√≥n", "certificaci√≥n", "aprobaci√≥n de equipos"],
        "refarming": ["refarming", "refarming de espectro", "reutilizaci√≥n"],
        "sandbox": ["sandbox", "entorno de prueba", "regulador experimental"],
        "transparencia": ["transparencia", "proceso transparente", "licitaci√≥n p√∫blica"],
        "competencia": ["competencia", "mercado competitivo", "promoci√≥n de competencia"],
        "proteccion_datos": ["protecci√≥n de datos", "datos personales", "privacidad"],
    }
    
    for tag, keywords in keyword_map.items():
        if any(keyword in text_lower for keyword in keywords):
            if tag not in tags:
                tags.append(tag)
    
    return tags

def extract_country_data(country_id, country_info):
    """Extrae los datos de un pa√≠s espec√≠fico"""
    url = f"{BASE_URL}/pagina/mejores-practicas-regulatorias/{country_info['url']}"
    
    print(f"\nExtrayendo datos de {country_info['name']}...")
    print(f"URL: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Buscar el contenido principal
        practices = {}
        
        # Buscar cada categor√≠a en el contenido
        for category in CATEGORIES:
            # Buscar el texto de la categor√≠a (puede estar en diferentes formatos)
            category_elem = soup.find(string=re.compile(category, re.I))
            
            if category_elem:
                # Buscar el contenido relacionado
                parent = category_elem.find_parent()
                content_text = ""
                
                # Intentar encontrar el siguiente p√°rrafo o div con contenido
                if parent:
                    # Buscar en los siguientes elementos
                    next_elements = parent.find_next_siblings()
                    for elem in next_elements[:3]:  # Revisar los siguientes 3 elementos
                        text = elem.get_text(strip=True)
                        if text and len(text) > 50:  # Si tiene contenido sustancial
                            content_text = text
                            break
                    
                    # Si no encontramos en siblings, buscar en el mismo elemento
                    if not content_text:
                        full_text = parent.get_text(strip=True)
                        # Extraer solo la parte despu√©s del t√≠tulo de la categor√≠a
                        parts = full_text.split(category, 1)
                        if len(parts) > 1:
                            content_text = parts[1].strip()
                
                # Si a√∫n no tenemos contenido, buscar en toda la p√°gina
                if not content_text:
                    # Buscar divs o secciones que puedan contener el contenido
                    all_text = soup.get_text()
                    # Buscar el patr√≥n: categor√≠a seguida de texto
                    pattern = re.compile(f"{re.escape(category)}[\\s\\S]{{0,500}}", re.I)
                    match = pattern.search(all_text)
                    if match:
                        content_text = match.group(0).replace(category, "").strip()
                
                if content_text and len(content_text) > 20:
                    # Limpiar el texto
                    content_text = re.sub(r'\s+', ' ', content_text)
                    summary = content_text[:200] + "..." if len(content_text) > 200 else content_text
                    
                    practices[category] = {
                        "summary": summary,
                        "details": content_text,
                        "tags": extract_tags(content_text)
                    }
                    print(f"  ‚úì {category}: {len(content_text)} caracteres")
                else:
                    print(f"  ‚úó {category}: No se encontr√≥ contenido")
                    practices[category] = {
                        "summary": "Informaci√≥n no disponible en la fuente.",
                        "details": "No se pudo extraer informaci√≥n espec√≠fica para esta categor√≠a desde la p√°gina web de REGULATEL.",
                        "tags": []
                    }
            else:
                print(f"  ‚úó {category}: No se encontr√≥ en la p√°gina")
                practices[category] = {
                    "summary": "Informaci√≥n no disponible en la fuente.",
                    "details": "No se pudo extraer informaci√≥n espec√≠fica para esta categor√≠a desde la p√°gina web de REGULATEL.",
                    "tags": []
                }
        
        return {
            "id": country_id,
            "name": country_info["name"],
            "flag": country_info["flag"],
            "practices": practices
        }
        
    except Exception as e:
        print(f"Error extrayendo datos de {country_info['name']}: {e}")
        return None

def main():
    """Funci√≥n principal"""
    print("=" * 60)
    print("EXTRACTOR DE DATOS DE REGULATEL")
    print("=" * 60)
    
    all_countries_data = []
    
    for country_id, country_info in COUNTRIES.items():
        data = extract_country_data(country_id, country_info)
        if data:
            all_countries_data.append(data)
        time.sleep(2)  # Esperar entre requests para no sobrecargar el servidor
    
    # Guardar los datos en un archivo JSON
    output_file = "data/regulatel_extracted_data.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_countries_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'=' * 60}")
    print(f"Datos extra√≠dos de {len(all_countries_data)} pa√≠ses")
    print(f"Archivo guardado en: {output_file}")
    print("=" * 60)
    
    # Mostrar resumen
    for country_data in all_countries_data:
        categories_found = sum(1 for p in country_data["practices"].values() 
                              if p["details"] != "No se pudo extraer informaci√≥n espec√≠fica")
        print(f"{country_data['name']}: {categories_found}/8 categor√≠as encontradas")

if __name__ == "__main__":
    main()


